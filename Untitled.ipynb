{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdcaa770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920a848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping and saving to Excel file completed.\n"
     ]
    }
   ],
   "source": [
    "def scrape_cnn_sport_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract title\n",
    "    title = soup.find('h1', class_='headline__text').get_text(strip=True) if soup.find('h1', class_='headline__text') else 'N/A'\n",
    "\n",
    "    # Extract publication time\n",
    "    timestamp_div = soup.find('div', class_='timestamp')\n",
    "    timestamp = timestamp_div.get_text(strip=True).replace('Published', '').strip() if timestamp_div else 'N/A'\n",
    "\n",
    "    # Extract author\n",
    "    author_div = soup.find('div', class_='byline__names')\n",
    "    author = author_div.find('span', class_='byline__name').get_text(strip=True) if author_div and author_div.find('span', class_='byline__name') else 'N/A'\n",
    "\n",
    "    # Extract first paragraph\n",
    "    first_paragraph = soup.find('p', class_='paragraph').get_text(strip=True) if soup.find('p', class_='paragraph') else 'N/A'\n",
    "\n",
    "    data = [[timestamp, title, first_paragraph, author]]\n",
    "    return data\n",
    "# Example URL of the article\n",
    "url = 'https://www.cnn.com/2024/07/23/sport/paris-olympics-security-threats-spt-intl/index.html'\n",
    "data = scrape_cnn_sport_article(url)\n",
    "\n",
    "# Create DataFrame and save to Excel\n",
    "df = pd.DataFrame(data, columns=['Date Time', 'News Title', 'First Paragraph', 'Author'])\n",
    "df.to_excel('sport.xlsx', index=False)\n",
    "\n",
    "print(\"Data scraping and saving to Excel file completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1036f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping and saving to Excel file completed.\n"
     ]
    }
   ],
   "source": [
    "def scrape_article_details(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('h1', class_='headline__text').get_text(strip=True) if soup.find('h1', class_='headline__text') else 'N/A'\n",
    "\n",
    "        # Extract publication time\n",
    "        timestamp_div = soup.find('div', class_='timestamp')\n",
    "        timestamp = timestamp_div.get_text(strip=True).replace('Published', '').strip() if timestamp_div else 'N/A'\n",
    "\n",
    "        # Extract author\n",
    "        author_div = soup.find('div', class_='byline__names')\n",
    "        author = author_div.find('span', class_='byline__name').get_text(strip=True) if author_div and author_div.find('span', class_='byline__name') else 'N/A'\n",
    "\n",
    "        # Extract first paragraph\n",
    "        first_paragraph = soup.find('p', class_='paragraph').get_text(strip=True) if soup.find('p', class_='paragraph') else 'N/A'\n",
    "\n",
    "        return [timestamp, title, first_paragraph, author]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "\n",
    "def scrape_cnn_sport():\n",
    "    url = 'https://www.cnn.com/sport'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all article links\n",
    "    article_links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if href.startswith('/2024') and '/sport' in href:\n",
    "            article_links.append('https://www.cnn.com' + href)\n",
    "    \n",
    "    data = []\n",
    "    for link in article_links:\n",
    "        data.append(scrape_article_details(link))\n",
    "        time.sleep(1)  # Wait for 1 second before next request to avoid overwhelming the server\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape data from the sport category\n",
    "data = scrape_cnn_sport()\n",
    "\n",
    "# Create DataFrame and save to Excel\n",
    "df = pd.DataFrame(data, columns=['Date Time', 'News Title', 'First Paragraph', 'Author'])\n",
    "df.to_excel('sport.xlsx', index=False)\n",
    "\n",
    "print(\"Data scraping and saving to Excel file completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_details(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('h1', class_='headline__text').get_text(strip=True) if soup.find('h1', class_='headline__text') else 'N/A'\n",
    "\n",
    "        # Extract publication time\n",
    "        timestamp_div = soup.find('div', class_='timestamp')\n",
    "        timestamp = timestamp_div.get_text(strip=True).replace('Published', '').strip() if timestamp_div else 'N/A'\n",
    "\n",
    "        # Extract author\n",
    "        author_div = soup.find('div', class_='byline__names')\n",
    "        author = author_div.find('span', class_='byline__name').get_text(strip=True) if author_div and author_div.find('span', class_='byline__name') else 'N/A'\n",
    "\n",
    "        # Extract first paragraph\n",
    "        first_paragraph = soup.find('p', class_='paragraph').get_text(strip=True) if soup.find('p', class_='paragraph') else 'N/A'\n",
    "\n",
    "        return [timestamp, title, first_paragraph, author]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "\n",
    "def scrape_cnn_football():\n",
    "    url = 'https://www.cnn.com/sport/football'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all article links\n",
    "    article_links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if href.startswith('/2024') and '/football' in href:\n",
    "            article_links.append('https://www.cnn.com' + href)\n",
    "    \n",
    "    data = []\n",
    "    for link in article_links:\n",
    "        data.append(scrape_article_details(link))\n",
    "        time.sleep(1)  # Wait for 1 second before next request to avoid overwhelming the server\n",
    "\n",
    "    return data\n",
    "\n",
    "# File path for the existing data\n",
    "file_path = 'sport.xlsx'\n",
    "\n",
    "# Scrape data from the football category\n",
    "new_data = scrape_cnn_football()\n",
    "\n",
    "# Load existing data if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    existing_df = pd.read_excel(file_path)\n",
    "    new_df = pd.DataFrame(new_data, columns=['Date Time', 'News Title', 'First Paragraph', 'Author'])\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    combined_df = pd.DataFrame(new_data, columns=['Date Time', 'News Title', 'First Paragraph', 'Author'])\n",
    "\n",
    "# Save the combined data to Excel\n",
    "combined_df.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"Data scraping and saving to Excel file completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92da953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_env]",
   "language": "python",
   "name": "conda-env-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
